import streamlit as st

import os #ë°ì´í„° ê²½ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬

import pandas as pd
import numpy as np # ìˆ˜ì¹˜ ë‹¤ë£¨ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬


import re # í…ìŠ¤íŠ¸ í•¸ë“¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬

import tensorflow as tf # í…ì„œí”Œë¡œìš°
import chatbot_model as chat # ë‚´ê°€ ë§Œë“  ëª¨ë¸
import tensorflow_datasets as tfds # Subword í† í¬ë‚˜ì´ì € ë¼ì´ë¸ŒëŸ¬ë¦¬




st.title('ğŸ™‚ ìœ„ë¡œë¥¼ ê±´ë„¤ëŠ” ì±—ë´‡')

st.write("í˜ë“  ì¼ì´ ìˆìœ¼ì‹œë©´, ì±—ë´‡ì—ê²Œ ë§ì„ ê±¸ì–´ë³´ì•„ìš”..")


title = st.text_input('ì±—ë´‡ì—ê²Œ : ', value="ë‚˜ ì˜¤ëŠ˜ ë„ˆë¬´ ìš°ìš¸í•´... ğŸ˜­")
st.write("ì±—ë´‡ì˜ í•œë§ˆë”” :")

#ëª¨ë¸
tf.keras.backend.clear_session()

# í•˜ì´í¼íŒŒë¼ë¯¸í„°
NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜
D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›
NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ 
UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°, ë…¸ë“œ
DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨
VOCAB_SIZE = 6360

model = chat.transformer(
    vocab_size=VOCAB_SIZE,
    num_layers=NUM_LAYERS,
    units=UNITS,
    d_model=D_MODEL,
    num_heads=NUM_HEADS,
    dropout=DROPOUT)

model.load_weights('./my_model_weights')




corpus = []
with open('tokenize.subwords', 'r', encoding='utf-8') as f:
   for inx, line in enumerate(f):
       if inx > 1:
          sent = re.sub(r"$[_]+", " ", line)
          sent = line.replace('\n', '')
          corpus.append(sent)

def process(item):
    if item[-2] == '_':
        return (item[1:-2] + ' ')
    else:
        return item[1:-1]
corpus2 = list(map(process, corpus))

tokenizer = tfds.deprecated.text.SubwordTextEncoder(vocab_list = corpus2)


def decoder_inference(sentence):
    sentence = chat.preprocess_sentence(sentence)

    START_TOKEN, END_TOKEN = [6358], [6359]
    MAX_LENGTH = 16

  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.
  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]
    sentence = tf.expand_dims(
      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)

  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.
  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331
    output_sequence = tf.expand_dims(START_TOKEN, 0)

  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„
    for i in range(MAX_LENGTH):
    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.
        predictions = model(inputs=[sentence, output_sequence], training=False)
        predictions = predictions[:, -1:, :]

    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜
        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)

    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ
        if tf.equal(predicted_id, END_TOKEN[0]):
            break

    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.
    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.
        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)

    return tf.squeeze(output_sequence, axis=0)

def sentence_generation(sentence):
      # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.
    prediction = decoder_inference(sentence)

  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    predicted_sentence = tokenizer.decode(
      [i for i in prediction if i < tokenizer.vocab_size])

    print('ì…ë ¥ : {}'.format(sentence))
    print('ì¶œë ¥ : {}'.format(predicted_sentence))

    return predicted_sentence 




st.write(sentence_generation(title))
