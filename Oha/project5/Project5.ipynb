{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='background-color: #fff5b1'>ì˜¤ëŠ˜ì˜ í•œ ê±¸ìŒ  ğŸš¶ğŸ½â€â™‚ï¸: ë§ˆìŒì„ ë‚˜ëˆ„ëŠ” ì±—ë´‡</span>\n",
    "\n",
    "## Contexts\n",
    "\n",
    "### 1. READY\n",
    "    1-1 ì˜¤ëŠ˜ì˜ Expì™€ Rubric  \n",
    "    1-2 ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬  \n",
    "\n",
    "### 2. GAME\n",
    "    2-1. ë°ì´í„° ì½ì–´ì˜¤ê¸°  \n",
    "    2-2. ë°ì´í„° ì „ì²˜ë¦¬  \n",
    "    2-3. ëª¨ë¸ í•™ìŠµ  \n",
    "        2-3-1. split\n",
    "        2-3-2. model (transformer)\n",
    "    2-4. ë°ì´í„° í‰ê°€   \n",
    "\n",
    "### 3. POTG\n",
    "    3-1. ì†Œê°\n",
    "    3-2. ì–´ë ¤ì› ë˜ ì ê³¼ ê·¹ë³µë°©ì•ˆ  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ready\n",
    "## 1-1. ì˜¤ëŠ˜ì˜ Expì™€ Rubric\n",
    "\n",
    "ì‚¬ì‹¤ ìµœê·¼ì— GPT chat bot ì´ë¼ëŠ” ê¸°ê°€ë§‰íŒ ì±—ë´‡ì´ ìˆë‹¤.  \n",
    "ìš°ë¦¬ëŠ” í•™ìŠµì¤‘ì´ê¸° ë•Œë¬¸ì— ê·¸ëŸ° ì„±ëŠ¥ì„ ë‚´ê¸° ë³´ë‹¤ëŠ”  \n",
    "ì „ì²´ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ ë‹¤ë£¨ê³   \n",
    "ì–´ë–»ê²Œ í•™ìŠµì‹œí‚¤ëŠ”ì§€ë¥¼ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¨ê¸°ë¡œ í•œë‹¤.\n",
    "\n",
    "ë°ì´í„°ì…‹ì€ AI Hub ì— ìˆëŠ”  \n",
    "wellness ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì˜€ë‹¤.\n",
    "\n",
    "<a href=\"https://aihub.or.kr/unitysearch/list.do?kwd=%EC%9B%B0%EB%8B%88%EC%8A%A4\">ì›°ë‹ˆìŠ¤ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„°ì…‹</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. ì‚¬ìš©í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #ë°ì´í„° ê²½ë¡œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # ìˆ˜ì¹˜ ë‹¤ë£¨ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "\n",
    "import re # í…ìŠ¤íŠ¸ í•¸ë“¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n",
    "import tensorflow as tf # í…ì„œí”Œë¡œìš°\n",
    "import tensorflow_datasets as tfds # Subword í† í¬ë‚˜ì´ì € ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GAME\n",
    "## 2-1. ë°ì´í„° ì½ì–´ì˜¤ê¸°"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë°ì´í„°ëŠ” ë‘ ê°œì˜ ì•¡ì…€ ë°ì´í„°ë¥¼ ì œê³µí•˜ê³  ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>í•µì‹¬ì¦ìƒ</th>\n",
       "      <th>intent</th>\n",
       "      <th>keyword(ì„ìƒí‚¤ì›Œë“œ)</th>\n",
       "      <th>íŠ¹ì´ì‚¬í•­</th>\n",
       "      <th>ì—°ê´€í‘œí˜„</th>\n",
       "      <th>utterance</th>\n",
       "      <th>utterance(2ì°¨)</th>\n",
       "      <th>response(ê³µê°)</th>\n",
       "      <th>ì„ìƒì§ˆë¬¸ê·¸ë£¹(ì—°ì„¸ì˜ë£Œì›ì œê³µ)</th>\n",
       "      <th>utterance(ê¸ì •)</th>\n",
       "      <th>utterance(ë¶€ì •)</th>\n",
       "      <th>ê¸ì •ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€</th>\n",
       "      <th>ë¶€ì •ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€</th>\n",
       "      <th>ì¶”ê°€ë°œí™”(190917)</th>\n",
       "      <th>ì¶”ê°€ë°œí™” (191031)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì§€ì†ë˜ëŠ”ìš°ìš¸í•œê¸°ë¶„(ìš°ìš¸ê°)</td>\n",
       "      <td>ì •ì‹ ì¦ìƒ/ìš°ìš¸ê°</td>\n",
       "      <td>ìš°ìš¸</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ë¨¸ë¦¬ê°€ ì§“ëˆŒëŸ¬ì§€ëŠ” ëŠë‚Œ/ë¨¸ë¦¬ê°€ ë§‘ì§€ ì•Šë‹¤/ì¹¨ìš¸í•˜ë©´ì„œ ì ë„ ëª» ìë‹¤/í›„íšŒê°€ ë§ê³  ì¹¨ìš¸...</td>\n",
       "      <td>ìš°ìš¸í•´</td>\n",
       "      <td>ì„ì‹ í•´ì„œ ìš°ìš¸í•´</td>\n",
       "      <td>ê¸°ë¶„ì´ ìš°ìš¸í•˜ì‹œêµ°ìš”. 00ë‹˜ì—ê²Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ëŠ” ì¼ì´ ìˆì—ˆë˜ ê±´ ì•„ë‹Œì§€ ê±±ì •ìŠ¤ëŸ¬ì›Œìš”.</td>\n",
       "      <td>ê±°ì˜ ë§¤ì¼, í•˜ë£¨ ì¤‘ ëŒ€ë¶€ë¶„ ì‹œê°„ì„ ìš°ìš¸í•œ ê¸°ë¶„ìœ¼ë¡œ ìˆê±°ë‚˜ ê°€ë¼ì•‰ì•˜ë˜ ì ì´ ìˆë‚˜ìš”?</td>\n",
       "      <td>ì‘</td>\n",
       "      <td>ì•„ë‹ˆ</td>\n",
       "      <td>ì €ëŸ°â€¦ (ìš°ëŠ” ì´ëª¨í‹°ì½˜) ê¸°ë¶„ì´ ë‚˜ì•„ì§€ì§€ ì•ŠëŠ” ìƒíƒœêµ°ìš”.</td>\n",
       "      <td>ê·¸ë ‡êµ°ìš”. ê¸°ë¶„ì´ ê³„ì† ì²˜ì§€ì‹ ë‹¤ë©´ í¸ì•ˆí•œ ìŒì•…ì„ í•œë²ˆ ë“¤ì–´ë³´ëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             í•µì‹¬ì¦ìƒ    intent keyword(ì„ìƒí‚¤ì›Œë“œ) íŠ¹ì´ì‚¬í•­  \\\n",
       "0  ì§€ì†ë˜ëŠ”ìš°ìš¸í•œê¸°ë¶„(ìš°ìš¸ê°)  ì •ì‹ ì¦ìƒ/ìš°ìš¸ê°             ìš°ìš¸  NaN   \n",
       "\n",
       "                                                ì—°ê´€í‘œí˜„ utterance utterance(2ì°¨)   \\\n",
       "0  ë¨¸ë¦¬ê°€ ì§“ëˆŒëŸ¬ì§€ëŠ” ëŠë‚Œ/ë¨¸ë¦¬ê°€ ë§‘ì§€ ì•Šë‹¤/ì¹¨ìš¸í•˜ë©´ì„œ ì ë„ ëª» ìë‹¤/í›„íšŒê°€ ë§ê³  ì¹¨ìš¸...       ìš°ìš¸í•´       ì„ì‹ í•´ì„œ ìš°ìš¸í•´   \n",
       "\n",
       "                                     response(ê³µê°)  \\\n",
       "0  ê¸°ë¶„ì´ ìš°ìš¸í•˜ì‹œêµ°ìš”. 00ë‹˜ì—ê²Œ ìŠ¤íŠ¸ë ˆìŠ¤ ë°›ëŠ” ì¼ì´ ìˆì—ˆë˜ ê±´ ì•„ë‹Œì§€ ê±±ì •ìŠ¤ëŸ¬ì›Œìš”.   \n",
       "\n",
       "                                  ì„ìƒì§ˆë¬¸ê·¸ë£¹(ì—°ì„¸ì˜ë£Œì›ì œê³µ) utterance(ê¸ì •) utterance(ë¶€ì •)  \\\n",
       "0  ê±°ì˜ ë§¤ì¼, í•˜ë£¨ ì¤‘ ëŒ€ë¶€ë¶„ ì‹œê°„ì„ ìš°ìš¸í•œ ê¸°ë¶„ìœ¼ë¡œ ìˆê±°ë‚˜ ê°€ë¼ì•‰ì•˜ë˜ ì ì´ ìˆë‚˜ìš”?             ì‘            ì•„ë‹ˆ   \n",
       "\n",
       "                        ê¸ì •ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€  \\\n",
       "0  ì €ëŸ°â€¦ (ìš°ëŠ” ì´ëª¨í‹°ì½˜) ê¸°ë¶„ì´ ë‚˜ì•„ì§€ì§€ ì•ŠëŠ” ìƒíƒœêµ°ìš”.     \n",
       "\n",
       "                                      ë¶€ì •ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€  ì¶”ê°€ë°œí™”(190917)  \\\n",
       "0  ê·¸ë ‡êµ°ìš”. ê¸°ë¶„ì´ ê³„ì† ì²˜ì§€ì‹ ë‹¤ë©´ í¸ì•ˆí•œ ìŒì•…ì„ í•œë²ˆ ë“¤ì–´ë³´ëŠ” ê²ƒë„ ì¢‹ì„ ê²ƒ ê°™ì•„ìš”.           NaN   \n",
       "\n",
       "   ì¶”ê°€ë°œí™” (191031)  \n",
       "0            NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filepath = './ì›°ë‹ˆìŠ¤ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„°ì…‹/02)ì›°ë‹ˆìŠ¤_ëŒ€í™”_ìŠ¤í¬ë¦½íŠ¸_ë°ì´í„°ì…‹.xlsx'\n",
    "df = pd.read_excel(dataset_filepath, engine='openpyxl')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>êµ¬ë¶„</th>\n",
       "      <th>ìœ ì €</th>\n",
       "      <th>ì±—ë´‡</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ</td>\n",
       "      <td>ì œ ê°ì •ì´ ì´ìƒí•´ì§„ ê²ƒ ê°™ì•„ìš”. ë‚¨í¸ë§Œ ë³´ë©´ í™”ê°€ ì¹˜ë°€ì–´ ì˜¤ë¥´ê³  ê°ì • ì¡°ì ˆì´ ì•ˆë˜ìš”.</td>\n",
       "      <td>ê°ì •ì´ ì¡°ì ˆì´ ì•ˆ ë  ë•Œë§Œí¼ í˜ë“¤ ë•ŒëŠ” ì—†ëŠ” ê±° ê°™ì•„ìš”.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ</td>\n",
       "      <td>ë” ì´ìƒ ë‚´ ê°ì •ì„ ë‚´ê°€ ì»¨íŠ¸ë¡¤ ëª» í•˜ê² ì–´.</td>\n",
       "      <td>ì €ë„ ê·¸ ê¸°ë¶„ ì´í•´í•´ìš”. ë§ì´ í˜ë“œì‹œì£ ?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ</td>\n",
       "      <td>í•˜ë£¨ì¢…ì¼ ì˜¤ë¥´ë½ë‚´ë¦¬ë½ ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê¸°ë¶„ì´ì—ìš”.</td>\n",
       "      <td>ê·¸ëŸ´ ë•ŒëŠ” ë°¥ì€ ì˜ ë¨¹ì—ˆëŠ”ì§€, ì ì€ ì˜ ì¤ëŠ”ì§€ ì²´í¬í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš”.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ</td>\n",
       "      <td>ê¼­ ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê²ƒ ê°™ì•„ìš”.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ</td>\n",
       "      <td>ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê²ƒì²˜ëŸ¼ ê¸°ë¶„ì´ ì™”ë‹¤ ê°”ë‹¤ í•´ìš”.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          êµ¬ë¶„                                               ìœ ì €  \\\n",
       "0  ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ  ì œ ê°ì •ì´ ì´ìƒí•´ì§„ ê²ƒ ê°™ì•„ìš”. ë‚¨í¸ë§Œ ë³´ë©´ í™”ê°€ ì¹˜ë°€ì–´ ì˜¤ë¥´ê³  ê°ì • ì¡°ì ˆì´ ì•ˆë˜ìš”.   \n",
       "1  ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ                         ë” ì´ìƒ ë‚´ ê°ì •ì„ ë‚´ê°€ ì»¨íŠ¸ë¡¤ ëª» í•˜ê² ì–´.   \n",
       "2  ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ                      í•˜ë£¨ì¢…ì¼ ì˜¤ë¥´ë½ë‚´ë¦¬ë½ ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê¸°ë¶„ì´ì—ìš”.   \n",
       "3  ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ                                ê¼­ ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê²ƒ ê°™ì•„ìš”.   \n",
       "4  ê°ì •/ê°ì •ì¡°ì ˆì´ìƒ                       ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê²ƒì²˜ëŸ¼ ê¸°ë¶„ì´ ì™”ë‹¤ ê°”ë‹¤ í•´ìš”.   \n",
       "\n",
       "                                        ì±—ë´‡  Unnamed: 3  \n",
       "0          ê°ì •ì´ ì¡°ì ˆì´ ì•ˆ ë  ë•Œë§Œí¼ í˜ë“¤ ë•ŒëŠ” ì—†ëŠ” ê±° ê°™ì•„ìš”.         NaN  \n",
       "1                   ì €ë„ ê·¸ ê¸°ë¶„ ì´í•´í•´ìš”. ë§ì´ í˜ë“œì‹œì£ ?         NaN  \n",
       "2  ê·¸ëŸ´ ë•ŒëŠ” ë°¥ì€ ì˜ ë¨¹ì—ˆëŠ”ì§€, ì ì€ ì˜ ì¤ëŠ”ì§€ ì²´í¬í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš”.         NaN  \n",
       "3                                      NaN         NaN  \n",
       "4                                      NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filepath = './ì›°ë‹ˆìŠ¤ ëŒ€í™” ìŠ¤í¬ë¦½íŠ¸ ë°ì´í„°ì…‹/ì›°ë‹ˆìŠ¤_ëŒ€í™”_ìŠ¤í¬ë¦½íŠ¸_ë°ì´í„°ì…‹.xlsx'\n",
    "df2 = pd.read_excel(dataset_filepath, engine='openpyxl')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ìœ„ì™€ ê°™ì´ í•˜ë‚˜ëŠ” ì§ˆë¬¸ê³¼ ë‹µë³€, ê·¸ë¦¬ê³  ê·¸ì— ëŒ€í•œ ì„¸ë¶€ ì •ë³´ë“¤ë¡œ ì´ë£¨ì–´ì ¸ ìˆê³ ,  \n",
    "ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ì§ˆë¬¸ê³¼ ì±—ë´‡ì˜ ë‹µë³€ ë°ì´í„°ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['í•µì‹¬ì¦ìƒ', 'intent', 'keyword(ì„ìƒí‚¤ì›Œë“œ)', 'íŠ¹ì´ì‚¬í•­', 'ì—°ê´€í‘œí˜„', 'utterance',\n",
       "       'utterance(2ì°¨) ', 'response(ê³µê°)', 'ì„ìƒì§ˆë¬¸ê·¸ë£¹(ì—°ì„¸ì˜ë£Œì›ì œê³µ)', 'utterance(ê¸ì •)',\n",
       "       'utterance(ë¶€ì •)', 'ê¸ì •ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€', 'ë¶€ì •ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€', 'ì¶”ê°€ë°œí™”(190917)',\n",
       "       'ì¶”ê°€ë°œí™” (191031)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "íŠ¹íˆ ì²«ë²ˆì§¸ ìë£Œì˜ ê²½ìš° ìƒë‹¹í•œ ì–‘ì˜ ìë£Œê°€ ìˆì§€ë§Œ, ì•„ì‰½ê²Œë„ êµ°ë°êµ°ë°  \n",
    "ë¹ ì§„ ë°ì´í„°ê°€ ë§ì•˜ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296535"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "í•µì‹¬ì¦ìƒ               19759\n",
       "intent             19750\n",
       "keyword(ì„ìƒí‚¤ì›Œë“œ)     19555\n",
       "íŠ¹ì´ì‚¬í•­               19736\n",
       "ì—°ê´€í‘œí˜„               19747\n",
       "utterance          15452\n",
       "utterance(2ì°¨)         92\n",
       "response(ê³µê°)       19695\n",
       "ì„ìƒì§ˆë¬¸ê·¸ë£¹(ì—°ì„¸ì˜ë£Œì›ì œê³µ)    19634\n",
       "utterance(ê¸ì •)      17069\n",
       "utterance(ë¶€ì •)      17069\n",
       "ê¸ì •ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€       19715\n",
       "ë¶€ì •ì— ëŒ€í•œ ì±—ë´‡ ë‹µë³€       19715\n",
       "ì¶”ê°€ë°œí™”(190917)       19769\n",
       "ì¶”ê°€ë°œí™” (191031)      19769\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19769, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê·¸ëŸ¬ë‚˜ ì´ 19769ê°œì˜ í–‰ ìˆ˜ì¤‘ ë‹¤ë¥¸ ê²ƒë“¤ì€ 90%ê°€ ê²°ì¸¡ì¹˜ë¡œ ì¡´ì¬í•˜ì—¬  \n",
    "í•œ ìŒìœ¼ë¡œ ë¬¶ì–´ ì‚¬ìš©í•˜ê¸° ì–´ë µë‹¤ê³  ìƒê°í–ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5231, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "êµ¬ë¶„               0\n",
       "ìœ ì €               0\n",
       "ì±—ë´‡            4197\n",
       "Unnamed: 3    5231\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ì´ì— ë°˜í•´ ë‘ë²ˆì§¸ ë°ì´í„°ì…‹ì€ ìœ ì €ì™€ ì±—ë´‡ì˜ ìŒ ë°ì´í„°ê°€  \n",
    "1000ì¢… ìˆì–´ ì´ê²ƒìœ¼ë¡œ í•™ìŠµì„ í•˜ê¸°ë¡œ ê²°ì •í–ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2[[\"ìœ ì €\", \"ì±—ë´‡\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ìœ ì €</th>\n",
       "      <th>ì±—ë´‡</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ì œ ê°ì •ì´ ì´ìƒí•´ì§„ ê²ƒ ê°™ì•„ìš”. ë‚¨í¸ë§Œ ë³´ë©´ í™”ê°€ ì¹˜ë°€ì–´ ì˜¤ë¥´ê³  ê°ì • ì¡°ì ˆì´ ì•ˆë˜ìš”.</td>\n",
       "      <td>ê°ì •ì´ ì¡°ì ˆì´ ì•ˆ ë  ë•Œë§Œí¼ í˜ë“¤ ë•ŒëŠ” ì—†ëŠ” ê±° ê°™ì•„ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ë” ì´ìƒ ë‚´ ê°ì •ì„ ë‚´ê°€ ì»¨íŠ¸ë¡¤ ëª» í•˜ê² ì–´.</td>\n",
       "      <td>ì €ë„ ê·¸ ê¸°ë¶„ ì´í•´í•´ìš”. ë§ì´ í˜ë“œì‹œì£ ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>í•˜ë£¨ì¢…ì¼ ì˜¤ë¥´ë½ë‚´ë¦¬ë½ ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê¸°ë¶„ì´ì—ìš”.</td>\n",
       "      <td>ê·¸ëŸ´ ë•ŒëŠ” ë°¥ì€ ì˜ ë¨¹ì—ˆëŠ”ì§€, ì ì€ ì˜ ì¤ëŠ”ì§€ ì²´í¬í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>í‰ì†Œ ë‹¤ë¥¸ ì¼ì„ í•  ë•Œë„ ë¹„ìŠ·í•´ìš”. ìƒê°í•œëŒ€ë¡œ ì•ˆë˜ë©´ í™”ê°€ ë‚˜ê³ â€¦ê·¸ëŸ° ìƒí™©ì´ ì§€ì†ë˜...</td>\n",
       "      <td>í™”ê°€ í­ë°œí•  ê²ƒ ê°™ì„ ë•ŒëŠ” ê·¸ ìë¦¬ë¥¼ í”¼í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ë¼ê³  ìƒê°í•´ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ì˜ˆì „ë³´ë‹¤ í™”ë‚´ëŠ” ê²Œ ê³¼ê²©í•´ì§„ ê±° ê°™ì•„.</td>\n",
       "      <td>ì •ë§ í˜ë“œì‹œê² ì–´ìš”. í™”ëŠ” ë‚¨ì—ê²Œë„ ìŠ¤ìŠ¤ë¡œì—ê²Œë„ ìƒì²˜ë¥¼ ì£¼ì–ì•„ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>ê·¸ ì‚¬ëŒì´ ì‘ê¸‰ì‹¤ ì˜ì‚¬í•œí…Œ ë­ë¼ê³  ì†ë‹¥ê±°ë¦¬ë‹ˆê¹Œ, ì €ë³´ê³  ê°‘ìê¸° ì‘ê¸‰ì²˜ì¹˜ ëë‚¬ë‹¤ê³ , ...</td>\n",
       "      <td>ì‘ê¸‰ì‹¤ì´ ìˆì–´ì„œ ë‹¤í–‰ì´ë„¤ìš”. í° ë¬¸ì œëŠ” ì—†ìœ¼ì‹  ê±°ì£ ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>íŒŒí¸ì´ íŠ€ì–´ì„œ ê·¸ ë•Œ ì €ë„ ì‘ê¸‰ì‹¤ ê°€ì„œ ì¹˜ë£Œ ë°›ê¸°ë„ í–ˆê³ â€¦</td>\n",
       "      <td>ì‘ê¸‰ì‹¤ì— ê°€ì…¨ë‹¤ë‹ˆ ì •ë§ ë†€ëì–´ìš”. ì•„ë¬´ ë¬¸ì œ ì—†ìœ¼ì‹ ê°€ìš”? ê±±ì • ë˜ë„¤ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>ì§€ê¸ˆ ìƒíƒœê°€ ë„ˆë¬´ ì•ˆ ì¢‹ì•„ì„œ í•™êµ ì•ˆ ë‚˜ê°€ê³  ìˆì–´ìš”.</td>\n",
       "      <td>ìƒíƒœê°€ ë” ì•ˆ ì¢‹ì•„ì§€ì…¨êµ°ìš”. ê±±ì •ì´ ë˜ë„¤ìš”.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>ì§„ì§œ ì‹¬í•´ì§„ ê±° ê°™ê¸´ í•´ìš”.</td>\n",
       "      <td>ì •ë§ í˜ë“œì‹œê² ì–´ìš”. ì§€ê¸ˆë„ ì¦ìƒì´ ì‹¬í•˜ì‹ ê°€ìš”?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>ê·¸ëŸ°ë° ì¦ìƒì´ ë‚˜ë¹ ì§„ ê±° ê°™ì•„.</td>\n",
       "      <td>ë„ˆë¬´ ì‹¬í•˜ì‹œë©´ ë³‘ì›ì„ ë‹¤ì‹œ ê°€ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1034 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     ìœ ì €  \\\n",
       "0       ì œ ê°ì •ì´ ì´ìƒí•´ì§„ ê²ƒ ê°™ì•„ìš”. ë‚¨í¸ë§Œ ë³´ë©´ í™”ê°€ ì¹˜ë°€ì–´ ì˜¤ë¥´ê³  ê°ì • ì¡°ì ˆì´ ì•ˆë˜ìš”.   \n",
       "1                              ë” ì´ìƒ ë‚´ ê°ì •ì„ ë‚´ê°€ ì»¨íŠ¸ë¡¤ ëª» í•˜ê² ì–´.   \n",
       "2                           í•˜ë£¨ì¢…ì¼ ì˜¤ë¥´ë½ë‚´ë¦¬ë½ ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê¸°ë¶„ì´ì—ìš”.   \n",
       "15    í‰ì†Œ ë‹¤ë¥¸ ì¼ì„ í•  ë•Œë„ ë¹„ìŠ·í•´ìš”. ìƒê°í•œëŒ€ë¡œ ì•ˆë˜ë©´ í™”ê°€ ë‚˜ê³ â€¦ê·¸ëŸ° ìƒí™©ì´ ì§€ì†ë˜...   \n",
       "16                                ì˜ˆì „ë³´ë‹¤ í™”ë‚´ëŠ” ê²Œ ê³¼ê²©í•´ì§„ ê±° ê°™ì•„.   \n",
       "...                                                 ...   \n",
       "5196  ê·¸ ì‚¬ëŒì´ ì‘ê¸‰ì‹¤ ì˜ì‚¬í•œí…Œ ë­ë¼ê³  ì†ë‹¥ê±°ë¦¬ë‹ˆê¹Œ, ì €ë³´ê³  ê°‘ìê¸° ì‘ê¸‰ì²˜ì¹˜ ëë‚¬ë‹¤ê³ , ...   \n",
       "5197                   íŒŒí¸ì´ íŠ€ì–´ì„œ ê·¸ ë•Œ ì €ë„ ì‘ê¸‰ì‹¤ ê°€ì„œ ì¹˜ë£Œ ë°›ê¸°ë„ í–ˆê³ â€¦   \n",
       "5213                     ì§€ê¸ˆ ìƒíƒœê°€ ë„ˆë¬´ ì•ˆ ì¢‹ì•„ì„œ í•™êµ ì•ˆ ë‚˜ê°€ê³  ìˆì–´ìš”.    \n",
       "5214                                   ì§„ì§œ ì‹¬í•´ì§„ ê±° ê°™ê¸´ í•´ìš”.    \n",
       "5215                                 ê·¸ëŸ°ë° ì¦ìƒì´ ë‚˜ë¹ ì§„ ê±° ê°™ì•„.    \n",
       "\n",
       "                                              ì±—ë´‡  \n",
       "0                ê°ì •ì´ ì¡°ì ˆì´ ì•ˆ ë  ë•Œë§Œí¼ í˜ë“¤ ë•ŒëŠ” ì—†ëŠ” ê±° ê°™ì•„ìš”.  \n",
       "1                         ì €ë„ ê·¸ ê¸°ë¶„ ì´í•´í•´ìš”. ë§ì´ í˜ë“œì‹œì£ ?  \n",
       "2        ê·¸ëŸ´ ë•ŒëŠ” ë°¥ì€ ì˜ ë¨¹ì—ˆëŠ”ì§€, ì ì€ ì˜ ì¤ëŠ”ì§€ ì²´í¬í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš”.  \n",
       "15    í™”ê°€ í­ë°œí•  ê²ƒ ê°™ì„ ë•ŒëŠ” ê·¸ ìë¦¬ë¥¼ í”¼í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ë¼ê³  ìƒê°í•´ìš”.  \n",
       "16           ì •ë§ í˜ë“œì‹œê² ì–´ìš”. í™”ëŠ” ë‚¨ì—ê²Œë„ ìŠ¤ìŠ¤ë¡œì—ê²Œë„ ìƒì²˜ë¥¼ ì£¼ì–ì•„ìš”.  \n",
       "...                                          ...  \n",
       "5196               ì‘ê¸‰ì‹¤ì´ ìˆì–´ì„œ ë‹¤í–‰ì´ë„¤ìš”. í° ë¬¸ì œëŠ” ì—†ìœ¼ì‹  ê±°ì£ ?  \n",
       "5197     ì‘ê¸‰ì‹¤ì— ê°€ì…¨ë‹¤ë‹ˆ ì •ë§ ë†€ëì–´ìš”. ì•„ë¬´ ë¬¸ì œ ì—†ìœ¼ì‹ ê°€ìš”? ê±±ì • ë˜ë„¤ìš”.  \n",
       "5213                    ìƒíƒœê°€ ë” ì•ˆ ì¢‹ì•„ì§€ì…¨êµ°ìš”. ê±±ì •ì´ ë˜ë„¤ìš”.  \n",
       "5214                   ì •ë§ í˜ë“œì‹œê² ì–´ìš”. ì§€ê¸ˆë„ ì¦ìƒì´ ì‹¬í•˜ì‹ ê°€ìš”?  \n",
       "5215                  ë„ˆë¬´ ì‹¬í•˜ì‹œë©´ ë³‘ì›ì„ ë‹¤ì‹œ ê°€ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”?  \n",
       "\n",
       "[1034 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filter = data.dropna()\n",
    "data_filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. ë°ì´í„° ì „ì²˜ë¦¬"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¨ì–´ë“¤ì„ ë”°ë¡œ ì…ë ¥í•˜ê¸° ìœ„í•´  \n",
    "ë¬¼ìŒí‘œ ë§ˆì¹¨í‘œ ë“±ì„ ë„ì›Œì£¼ëŠ” ì „ì²˜ë¦¬ ì‘ì—…ì„ ê±°ì³¤ë‹¤.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.strip() #ì–‘ ì˜†ì˜ ê³µë°± ì œê±°\n",
    "\n",
    "  # ë‹¨ì–´ì™€ êµ¬ë‘ì (punctuation) ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "  # ì˜ˆë¥¼ ë“¤ì–´ì„œ \"ë°°ê³ í”„ë‹¤.\" => \"ë°°ê³ í”„ë‹¤ .\"ì™€ ê°™ì´\n",
    "  # ì˜¨ì  ì‚¬ì´ì— ê±°ë¦¬ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "    sentence = re.sub(r\"([?.!,-])\", r\" \\1 \", sentence)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")ë¥¼ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ì¸ ' 'ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\n",
    "  # sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence) ì˜ì–´ì˜ ê²½ìš°\n",
    "    sentence = re.sub(r\"([a-zA-Z]+)\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # sentence = re.sub(r\"^[^ã„±-ã…ã…-ã…£ê°€-í£ ]\", \" \", sentence) #ì œëŒ€ë¡œ ëœ í•œêµ­ì–´ê°€ ì•„ë‹Œ ê²½ìš° ' ' ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²´ ìƒ˜í”Œ ìˆ˜, ì§ˆë¬¸ : 1034 , ëŒ€ë‹µ : 1034\n",
      "\n",
      "\n",
      "ì§ˆë¬¸ : ì œ ê°ì •ì´ ì´ìƒí•´ì§„ ê²ƒ ê°™ì•„ìš” . ë‚¨í¸ë§Œ ë³´ë©´ í™”ê°€ ì¹˜ë°€ì–´ ì˜¤ë¥´ê³  ê°ì • ì¡°ì ˆì´ ì•ˆë˜ìš” .\n",
      "ëŒ€ë‹µ : ê°ì •ì´ ì¡°ì ˆì´ ì•ˆ ë  ë•Œë§Œí¼ í˜ë“¤ ë•ŒëŠ” ì—†ëŠ” ê±° ê°™ì•„ìš” .\n",
      "\n",
      "ì§ˆë¬¸ : ë” ì´ìƒ ë‚´ ê°ì •ì„ ë‚´ê°€ ì»¨íŠ¸ë¡¤ ëª» í•˜ê² ì–´ .\n",
      "ëŒ€ë‹µ : ì €ë„ ê·¸ ê¸°ë¶„ ì´í•´í•´ìš” . ë§ì´ í˜ë“œì‹œì£  ?\n",
      "\n",
      "ì§ˆë¬¸ : í•˜ë£¨ì¢…ì¼ ì˜¤ë¥´ë½ë‚´ë¦¬ë½ ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê¸°ë¶„ì´ì—ìš” .\n",
      "ëŒ€ë‹µ : ê·¸ëŸ´ ë•ŒëŠ” ë°¥ì€ ì˜ ë¨¹ì—ˆëŠ”ì§€ , ì ì€ ì˜ ì¤ëŠ”ì§€ ì²´í¬í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš” .\n",
      "\n",
      "ì§ˆë¬¸ : í‰ì†Œ ë‹¤ë¥¸ ì¼ì„ í•  ë•Œë„ ë¹„ìŠ·í•´ìš” . ìƒê°í•œëŒ€ë¡œ ì•ˆë˜ë©´ í™”ê°€ ë‚˜ê³ â€¦ê·¸ëŸ° ìƒí™©ì´ ì§€ì†ë˜ë©´ í­ë°œí•´ë²„ë ¤ìš” .\n",
      "ëŒ€ë‹µ : í™”ê°€ í­ë°œí•  ê²ƒ ê°™ì„ ë•ŒëŠ” ê·¸ ìë¦¬ë¥¼ í”¼í•˜ëŠ” ê²ƒë„ ì¢‹ì€ ë°©ë²•ì´ë¼ê³  ìƒê°í•´ìš” .\n",
      "\n",
      "ì§ˆë¬¸ : ì˜ˆì „ë³´ë‹¤ í™”ë‚´ëŠ” ê²Œ ê³¼ê²©í•´ì§„ ê±° ê°™ì•„ .\n",
      "ëŒ€ë‹µ : ì •ë§ í˜ë“œì‹œê² ì–´ìš” . í™”ëŠ” ë‚¨ì—ê²Œë„ ìŠ¤ìŠ¤ë¡œì—ê²Œë„ ìƒì²˜ë¥¼ ì£¼ì–ì•„ìš” .\n",
      "\n",
      "ì§ˆë¬¸ : í™”ê°€ ì•ˆ ì°¸ì•„ì ¸ .\n",
      "ëŒ€ë‹µ : í™”ê°€ ë„ˆë¬´ ë§ì´ ë‚  ë•ŒëŠ” ì‹¬í˜¸í¡ì„ í•´ë³´ëŠ” ê²Œ ì–´ë–¨ê¹Œìš” ? ì” - í›„ -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = list(map(preprocess_sentence, data_filter['ìœ ì €']))\n",
    "A = list(map(preprocess_sentence, data_filter['ì±—ë´‡']))\n",
    "\n",
    "print(f'ì „ì²´ ìƒ˜í”Œ ìˆ˜, ì§ˆë¬¸ : {len(Q)} , ëŒ€ë‹µ : {len(A)}\\n\\n')\n",
    "\n",
    "cnt = 0\n",
    "for a, b in zip(Q,A):\n",
    "    print(f'ì§ˆë¬¸ : {a}')\n",
    "    print(f'ëŒ€ë‹µ : {b}')\n",
    "    print()\n",
    "    cnt += 1\n",
    "    if cnt > 5:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(Q + A, target_vocab_size=2**13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6358"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKENì˜ ë²ˆí˜¸ : [6358]\n",
      "END_TOKENì˜ ë²ˆí˜¸ : [6359]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6360\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size + 2 # start í† í°ê³¼ end í† í°ì„ ì¶”ê°€í•˜ì˜€ë‹¤.\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì§ˆë¬¸ : í•˜ë£¨ì¢…ì¼ ì˜¤ë¥´ë½ë‚´ë¦¬ë½ ë¡¤ëŸ¬ì½”ìŠ¤í„° íƒ€ëŠ” ê¸°ë¶„ì´ì—ìš” .\n",
      "ëŒ€ë‹µ : ê·¸ëŸ´ ë•ŒëŠ” ë°¥ì€ ì˜ ë¨¹ì—ˆëŠ”ì§€ , ì ì€ ì˜ ì¤ëŠ”ì§€ ì²´í¬í•´ë³´ëŠ” ê²ƒë„ ì¢‹ì•„ìš” .\n",
      "\n",
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ì²«ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: [227, 3325, 4830, 779, 1781, 1]\n",
      "ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ì²«ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: [34, 42, 4441, 20, 4681, 4, 604, 20, 2870, 2417, 23, 99, 1]\n"
     ]
    }
   ],
   "source": [
    "# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—…ì„ ìˆ˜í–‰.\n",
    "# ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜\n",
    "\n",
    "print(f'ì§ˆë¬¸ : {Q[2]}')\n",
    "print(f'ëŒ€ë‹µ : {A[2]}\\n')\n",
    "    \n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ì²«ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(Q[2])))\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ ì²«ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(A[2])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ê°™ì€ ë§ˆì¹¨í‘œëŠ” ê°™ì€ ìˆ«ìì¸ 1 ë¡œ í‘œê¸°ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. ëª¨ë¸ í•™ìŠµ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3-1. split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë¬¸ì¥ê¸¸ì´ í‰ê·  :  9.458413926499032\n",
      "ë¬¸ì¥ê¸¸ì´ ìµœëŒ€ :  25\n",
      "ë¬¸ì¥ê¸¸ì´ í‘œì¤€í¸ì°¨ :  3.63755819096933\n",
      "pad_sequences maxlen :  16\n",
      "ì „ì²´ ë¬¸ì¥ì˜ 0.960348162475822%ê°€ maxlen ì„¤ì •ê°’ ì´ë‚´ì— í¬í•¨ë©ë‹ˆë‹¤. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = Q + A\n",
    "\n",
    "total_data_text = [tokenizer.encode(text) for text in total_data_text]\n",
    "\n",
    "# í…ìŠ¤íŠ¸ë°ì´í„° ë¬¸ì¥ê¸¸ì´ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ìƒì„±í•œ í›„\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# ë¬¸ì¥ê¸¸ì´ì˜ í‰ê· ê°’, ìµœëŒ€ê°’, í‘œì¤€í¸ì°¨ë¥¼ ê³„ì‚°í•´ ë³¸ë‹¤. \n",
    "print('ë¬¸ì¥ê¸¸ì´ í‰ê·  : ', np.mean(num_tokens))\n",
    "print('ë¬¸ì¥ê¸¸ì´ ìµœëŒ€ : ', np.max(num_tokens))\n",
    "print('ë¬¸ì¥ê¸¸ì´ í‘œì¤€í¸ì°¨ : ', np.std(num_tokens))\n",
    "\n",
    "# ì˜ˆë¥¼ë“¤ì–´, ìµœëŒ€ ê¸¸ì´ë¥¼ (í‰ê·  + 2*í‘œì¤€í¸ì°¨)ë¡œ í•œë‹¤ë©´,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('ì „ì²´ ë¬¸ì¥ì˜ {}%ê°€ maxlen ì„¤ì •ê°’ ì´ë‚´ì— í¬í•¨ë©ë‹ˆë‹¤. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # ìµœëŒ€ ê¸¸ì´ 28 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "  \n",
    "    # ìµœëŒ€ ê¸¸ì´ 28ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë‹¨ì–´ì¥ì˜ í¬ê¸° : 6360\n",
      "í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: 1034\n",
      "í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: 1034\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(Q,A)\n",
    "\n",
    "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))\n",
    "print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(Q)))\n",
    "print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3-2. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "    # ê°ë„ ë°°ì—´ ìƒì„±\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "    # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sinê³¼ cosineì´ êµì°¨ë˜ë„ë¡ ì¬ë°°ì—´\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "      # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” Qì™€ Kì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "      # softmaxì ìš©\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "      # ìµœì¢… ì–´í…ì…˜ì€ ê°€ì¤‘ì¹˜ì™€ Vì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "    return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "# ì¸ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ë‘ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "        inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "      # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "        padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "      # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "        attention = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention\")({\n",
    "              'query': inputs,\n",
    "              'key': inputs,\n",
    "              'value': inputs,\n",
    "              'mask': padding_mask\n",
    "          })\n",
    "\n",
    "      # ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "        attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "        attention = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "      # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "        outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "        outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "      # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "        outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "        outputs = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "        return tf.keras.Model(\n",
    "          inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # ì„ë² ë”© ë ˆì´ì–´\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "    inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "\n",
    "# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜)\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "        })\n",
    "\n",
    "  # ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”\n",
    "  # Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # ì„ë² ë”© ë ˆì´ì–´\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "          units=units,\n",
    "          d_model=d_model,\n",
    "          num_heads=num_heads,\n",
    "          dropout=dropout,\n",
    "          name='decoder_layer_{}'.format(i),\n",
    "      )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "\n",
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "  # ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹\n",
    "  # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # ì¸ì½”ë”\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # ë””ì½”ë”\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µ\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, None, 256)    2682368     ['inputs[0][0]',                 \n",
      "                                                                  'enc_padding_mask[0][0]']       \n",
      "                                                                                                  \n",
      " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 256)    3209728     ['dec_inputs[0][0]',             \n",
      "                                                                  'encoder[0][0]',                \n",
      "                                                                  'look_ahead_mask[0][0]',        \n",
      "                                                                  'dec_padding_mask[0][0]']       \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, None, 6360)   1634520     ['decoder[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,526,616\n",
      "Trainable params: 7,526,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
    "D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
    "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
    "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°, ë…¸ë“œ\n",
    "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32) #íŒ¨ë”©ë§ˆìŠ¤í¬\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "     beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 17s 432ms/step - loss: 4.9961 - accuracy: 0.0616\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 6s 392ms/step - loss: 4.0996 - accuracy: 0.0824\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 5s 369ms/step - loss: 3.8326 - accuracy: 0.1325\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 5s 363ms/step - loss: 3.5484 - accuracy: 0.1576\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 5s 358ms/step - loss: 3.1805 - accuracy: 0.1925\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 5s 360ms/step - loss: 2.8175 - accuracy: 0.2247\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 5s 363ms/step - loss: 2.4900 - accuracy: 0.2624\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 5s 361ms/step - loss: 2.1783 - accuracy: 0.3078\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 5s 360ms/step - loss: 1.9014 - accuracy: 0.3556\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 1.6237 - accuracy: 0.4144\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 5s 369ms/step - loss: 1.3678 - accuracy: 0.4597\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 5s 368ms/step - loss: 1.1418 - accuracy: 0.4970\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 5s 371ms/step - loss: 0.9216 - accuracy: 0.5395\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 5s 363ms/step - loss: 0.7336 - accuracy: 0.5667\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 5s 365ms/step - loss: 0.5779 - accuracy: 0.5968\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 5s 382ms/step - loss: 0.4448 - accuracy: 0.6206\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 5s 365ms/step - loss: 0.3396 - accuracy: 0.6377\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 5s 359ms/step - loss: 0.2588 - accuracy: 0.6529\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 5s 367ms/step - loss: 0.1954 - accuracy: 0.6609\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 5s 366ms/step - loss: 0.1464 - accuracy: 0.6654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7c9092308>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4. ë°ì´í„° í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
    "  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
    "  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
    "    for i in range(MAX_LENGTH):\n",
    "    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "      # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('ì…ë ¥ : {}'.format(sentence))\n",
    "    print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ë‚˜ ë„ˆë¬´ í˜ë“¤ì–´\n",
      "ì¶œë ¥ : ì •ë§ ë‹¹í™©ìŠ¤ëŸ¬ìš°ì…¨ê² ì–´ìš” . í•˜ì§€ë§Œ ë„ˆë¬´ ë¬´ë¦¬í•´ì„œ ìƒê°í•´ë‚¼ í•„ìš”ëŠ” ì—†ë‹µë‹ˆë‹¤ .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì •ë§ ë‹¹í™©ìŠ¤ëŸ¬ìš°ì…¨ê² ì–´ìš” . í•˜ì§€ë§Œ ë„ˆë¬´ ë¬´ë¦¬í•´ì„œ ìƒê°í•´ë‚¼ í•„ìš”ëŠ” ì—†ë‹µë‹ˆë‹¤ .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ë‚˜ ë„ˆë¬´ í˜ë“¤ì–´')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ë¶€íŠ¸ìº í”„ ì•ˆí•˜ê³  ì‹¶ì–´ ì§‘ì— ê°ˆë˜\n",
      "ì¶œë ¥ : ê·¸ ë§ˆìŒ ë‹¤ ì´í•´í•´ìš” . ê·¸ëŸ´ ë• ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ìœ¼ë©´ ì¡°ê¸ˆ ë‚˜ì•„ì ¸ìš” .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ê·¸ ë§ˆìŒ ë‹¤ ì´í•´í•´ìš” . ê·¸ëŸ´ ë• ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ìœ¼ë©´ ì¡°ê¸ˆ ë‚˜ì•„ì ¸ìš” .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ë¶€íŠ¸ìº í”„ ì•ˆí•˜ê³  ì‹¶ì–´ ì§‘ì— ê°ˆë˜')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ë‹¤ë§Œ í˜„ì¬ ë“¤ì–´ê°€ìˆëŠ” ì •ë³´ë“¤ì€ ëª¨ë‘  \n",
    "ìœ„ë¡œí•´ ì£¼ëŠ” ë¬¸ë‹µì´ê¸° ë•Œë¬¸ì—  \n",
    "ì–´ë–¤ ë§ì„ í•´ë„ ìœ„ë¡œí•˜ëŠ” ë‹µë³€ë°–ì— ë‚˜ì˜¤ì§€ ì•ŠëŠ”ë‹¤ëŠ” í•œê³„ê°€ ìˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ë‚˜ ì˜¤ëŠ˜ í–‰ë³µí•´ ì‚¬ì‹¤\n",
      "ì¶œë ¥ : ì¼ì´ ë§ì•„ ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë§ê² ì–´ìš” . ëª¸ ìƒí•˜ì§€ ì•Šê²Œ ì˜ ì±™ê²¨ ë“œì„¸ìš” .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì¼ì´ ë§ì•„ ìŠ¤íŠ¸ë ˆìŠ¤ê°€ ë§ê² ì–´ìš” . ëª¸ ìƒí•˜ì§€ ì•Šê²Œ ì˜ ì±™ê²¨ ë“œì„¸ìš” .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ë‚˜ ì˜¤ëŠ˜ í–‰ë³µí•´ ì‚¬ì‹¤')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ë‚˜ ìŠ¤íŠ¸ë ˆìŠ¤ ì•ˆë§ì€ë””\n",
      "ì¶œë ¥ : ê·¸ ë§ˆìŒ ë‹¤ ì´í•´í•´ìš” . ê·¸ëŸ´ ë• ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ìœ¼ë©´ ì¡°ê¸ˆ ë‚˜ì•„ì ¸ìš” .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ê·¸ ë§ˆìŒ ë‹¤ ì´í•´í•´ìš” . ê·¸ëŸ´ ë• ë§›ìˆëŠ” ìŒì‹ì„ ë¨¹ìœ¼ë©´ ì¡°ê¸ˆ ë‚˜ì•„ì ¸ìš” .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ë‚˜ ìŠ¤íŠ¸ë ˆìŠ¤ ì•ˆë§ì€ë””')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. POTG\n",
    "## 3-1. ì†Œê°\n",
    "\n",
    "### \"ë„ˆë¬´ ë¶€ì¡±í•´ìš”,, ì´ì œ ì¢€ ë” ì˜ í•´ë³¼ê²Œìš” ğŸ¥¶\"\n",
    "ì§€ì‹ì´ ë„ˆë¬´ ì§§ìŒì„ ë§¤ì¼ ë§¤ì¼ ì‹¤ê°í•©ë‹ˆë‹¤.  \n",
    "ì•ìœ¼ë¡œë„ ë” ê³µë¶€ë¥¼ í•´ì•¼ í•  ê²ƒ ê°™ì•„ìš” ã… ã… "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. ì–´ë ¤ì› ë˜ ì ê³¼ ê·¹ë³µë°©ì•ˆ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ì—‘ì…€ íŒŒì¼ì„ ì—´ê¸° ìœ„í•´ì„œëŠ” 'xlrd' ë¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•˜ë‹¤.  \n",
    "ì‰½ê²Œ `pip install xlrd` ë¡œ ì„¤ì¹˜í•œ í›„ ì´ìš©í•´ ì£¼ì—ˆë‹¤.\n",
    "\n",
    "2. SubwordTextEncoder API ë¥¼ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œ  \n",
    " `tfds.deprecated.text` ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ë ¤ í–ˆìœ¼ë‚˜ ê³„ì† ì—ëŸ¬ê°€ ë‚¬ë‹¤.  \n",
    " ì´ìœ ëŠ” ë²„ì „ë¬¸ì œì˜€ê³  2.3+ ì´í•˜ ë²„ì „ì€ `tfds.features.text` ë¥¼ ì‚¬ìš©í•´ì•¼ í•œë‹¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4e373b8bba118244a329c5b4d875cb18b4349fdcd2551f3ef43ce953593cb0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
