{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style='background-color: #fff5b1'>오늘의 한 걸음  🚶🏽‍♂️: 마음을 나누는 챗봇</span>\n",
    "\n",
    "## Contexts\n",
    "\n",
    "### 1. READY\n",
    "    1-1 오늘의 Exp와 Rubric  \n",
    "    1-2 사용하는 라이브러리  \n",
    "\n",
    "### 2. GAME\n",
    "    2-1. 데이터 읽어오기  \n",
    "    2-2. 데이터 전처리  \n",
    "    2-3. 모델 학습  \n",
    "        2-3-1. split\n",
    "        2-3-2. model (transformer)\n",
    "    2-4. 데이터 평가   \n",
    "\n",
    "### 3. POTG\n",
    "    3-1. 소감\n",
    "    3-2. 어려웠던 점과 극복방안  \n",
    "\n",
    "---\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Ready\n",
    "## 1-1. 오늘의 Exp와 Rubric\n",
    "\n",
    "사실 최근에 GPT chat bot 이라는 기가막힌 챗봇이 있다.  \n",
    "우리는 학습중이기 때문에 그런 성능을 내기 보다는  \n",
    "전체적으로 텍스트데이터를 어떻게 다루고  \n",
    "어떻게 학습시키는지를 중점적으로 다루기로 한다.\n",
    "\n",
    "데이터셋은 AI Hub 에 있는  \n",
    "wellness 대화 스크립트 데이터셋을 이용하였다.\n",
    "\n",
    "<a href=\"https://aihub.or.kr/unitysearch/list.do?kwd=%EC%9B%B0%EB%8B%88%EC%8A%A4\">웰니스 대화 스크립트 데이터셋</a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1-2. 사용하는 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #데이터 경로 라이브러리\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np # 수치 다루는 라이브러리\n",
    "\n",
    "\n",
    "import re # 텍스트 핸들링 라이브러리\n",
    "\n",
    "import tensorflow as tf # 텐서플로우\n",
    "import tensorflow_datasets as tfds # Subword 토크나이저 라이브러리\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. GAME\n",
    "## 2-1. 데이터 읽어오기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터는 두 개의 액셀 데이터를 제공하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>핵심증상</th>\n",
       "      <th>intent</th>\n",
       "      <th>keyword(임상키워드)</th>\n",
       "      <th>특이사항</th>\n",
       "      <th>연관표현</th>\n",
       "      <th>utterance</th>\n",
       "      <th>utterance(2차)</th>\n",
       "      <th>response(공감)</th>\n",
       "      <th>임상질문그룹(연세의료원제공)</th>\n",
       "      <th>utterance(긍정)</th>\n",
       "      <th>utterance(부정)</th>\n",
       "      <th>긍정에 대한 챗봇 답변</th>\n",
       "      <th>부정에 대한 챗봇 답변</th>\n",
       "      <th>추가발화(190917)</th>\n",
       "      <th>추가발화 (191031)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>지속되는우울한기분(우울감)</td>\n",
       "      <td>정신증상/우울감</td>\n",
       "      <td>우울</td>\n",
       "      <td>NaN</td>\n",
       "      <td>머리가 짓눌러지는 느낌/머리가 맑지 않다/침울하면서 잠도 못 자다/후회가 많고 침울...</td>\n",
       "      <td>우울해</td>\n",
       "      <td>임신해서 우울해</td>\n",
       "      <td>기분이 우울하시군요. 00님에게 스트레스 받는 일이 있었던 건 아닌지 걱정스러워요.</td>\n",
       "      <td>거의 매일, 하루 중 대부분 시간을 우울한 기분으로 있거나 가라앉았던 적이 있나요?</td>\n",
       "      <td>응</td>\n",
       "      <td>아니</td>\n",
       "      <td>저런… (우는 이모티콘) 기분이 나아지지 않는 상태군요.</td>\n",
       "      <td>그렇군요. 기분이 계속 처지신다면 편안한 음악을 한번 들어보는 것도 좋을 것 같아요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             핵심증상    intent keyword(임상키워드) 특이사항  \\\n",
       "0  지속되는우울한기분(우울감)  정신증상/우울감             우울  NaN   \n",
       "\n",
       "                                                연관표현 utterance utterance(2차)   \\\n",
       "0  머리가 짓눌러지는 느낌/머리가 맑지 않다/침울하면서 잠도 못 자다/후회가 많고 침울...       우울해       임신해서 우울해   \n",
       "\n",
       "                                     response(공감)  \\\n",
       "0  기분이 우울하시군요. 00님에게 스트레스 받는 일이 있었던 건 아닌지 걱정스러워요.   \n",
       "\n",
       "                                  임상질문그룹(연세의료원제공) utterance(긍정) utterance(부정)  \\\n",
       "0  거의 매일, 하루 중 대부분 시간을 우울한 기분으로 있거나 가라앉았던 적이 있나요?             응            아니   \n",
       "\n",
       "                        긍정에 대한 챗봇 답변  \\\n",
       "0  저런… (우는 이모티콘) 기분이 나아지지 않는 상태군요.     \n",
       "\n",
       "                                      부정에 대한 챗봇 답변  추가발화(190917)  \\\n",
       "0  그렇군요. 기분이 계속 처지신다면 편안한 음악을 한번 들어보는 것도 좋을 것 같아요.           NaN   \n",
       "\n",
       "   추가발화 (191031)  \n",
       "0            NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filepath = './웰니스 대화 스크립트 데이터셋/02)웰니스_대화_스크립트_데이터셋.xlsx'\n",
    "df = pd.read_excel(dataset_filepath, engine='openpyxl')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>구분</th>\n",
       "      <th>유저</th>\n",
       "      <th>챗봇</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.</td>\n",
       "      <td>감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>더 이상 내 감정을 내가 컨트롤 못 하겠어.</td>\n",
       "      <td>저도 그 기분 이해해요. 많이 힘드시죠?</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>하루종일 오르락내리락 롤러코스터 타는 기분이에요.</td>\n",
       "      <td>그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>꼭 롤러코스터 타는 것 같아요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>감정/감정조절이상</td>\n",
       "      <td>롤러코스터 타는 것처럼 기분이 왔다 갔다 해요.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          구분                                               유저  \\\n",
       "0  감정/감정조절이상  제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.   \n",
       "1  감정/감정조절이상                         더 이상 내 감정을 내가 컨트롤 못 하겠어.   \n",
       "2  감정/감정조절이상                      하루종일 오르락내리락 롤러코스터 타는 기분이에요.   \n",
       "3  감정/감정조절이상                                꼭 롤러코스터 타는 것 같아요.   \n",
       "4  감정/감정조절이상                       롤러코스터 타는 것처럼 기분이 왔다 갔다 해요.   \n",
       "\n",
       "                                        챗봇  Unnamed: 3  \n",
       "0          감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.         NaN  \n",
       "1                   저도 그 기분 이해해요. 많이 힘드시죠?         NaN  \n",
       "2  그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.         NaN  \n",
       "3                                      NaN         NaN  \n",
       "4                                      NaN         NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_filepath = './웰니스 대화 스크립트 데이터셋/웰니스_대화_스크립트_데이터셋.xlsx'\n",
    "df2 = pd.read_excel(dataset_filepath, engine='openpyxl')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위와 같이 하나는 질문과 답변, 그리고 그에 대한 세부 정보들로 이루어져 있고,  \n",
    "다른 하나는 질문과 챗봇의 답변 데이터로 구성되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['핵심증상', 'intent', 'keyword(임상키워드)', '특이사항', '연관표현', 'utterance',\n",
       "       'utterance(2차) ', 'response(공감)', '임상질문그룹(연세의료원제공)', 'utterance(긍정)',\n",
       "       'utterance(부정)', '긍정에 대한 챗봇 답변', '부정에 대한 챗봇 답변', '추가발화(190917)',\n",
       "       '추가발화 (191031)'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "특히 첫번째 자료의 경우 상당한 양의 자료가 있지만, 아쉽게도 군데군데  \n",
    "빠진 데이터가 많았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296535"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "핵심증상               19759\n",
       "intent             19750\n",
       "keyword(임상키워드)     19555\n",
       "특이사항               19736\n",
       "연관표현               19747\n",
       "utterance          15452\n",
       "utterance(2차)         92\n",
       "response(공감)       19695\n",
       "임상질문그룹(연세의료원제공)    19634\n",
       "utterance(긍정)      17069\n",
       "utterance(부정)      17069\n",
       "긍정에 대한 챗봇 답변       19715\n",
       "부정에 대한 챗봇 답변       19715\n",
       "추가발화(190917)       19769\n",
       "추가발화 (191031)      19769\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19769, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 총 19769개의 행 수중 다른 것들은 90%가 결측치로 존재하여  \n",
    "한 쌍으로 묶어 사용하기 어렵다고 생각했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5231, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "구분               0\n",
       "유저               0\n",
       "챗봇            4197\n",
       "Unnamed: 3    5231\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이에 반해 두번째 데이터셋은 유저와 챗봇의 쌍 데이터가  \n",
    "1000종 있어 이것으로 학습을 하기로 결정했다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df2[[\"유저\", \"챗봇\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>유저</th>\n",
       "      <th>챗봇</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.</td>\n",
       "      <td>감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>더 이상 내 감정을 내가 컨트롤 못 하겠어.</td>\n",
       "      <td>저도 그 기분 이해해요. 많이 힘드시죠?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>하루종일 오르락내리락 롤러코스터 타는 기분이에요.</td>\n",
       "      <td>그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...</td>\n",
       "      <td>화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>예전보다 화내는 게 과격해진 거 같아.</td>\n",
       "      <td>정말 힘드시겠어요. 화는 남에게도 스스로에게도 상처를 주잖아요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5196</th>\n",
       "      <td>그 사람이 응급실 의사한테 뭐라고 속닥거리니까, 저보고 갑자기 응급처치 끝났다고, ...</td>\n",
       "      <td>응급실이 있어서 다행이네요. 큰 문제는 없으신 거죠?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5197</th>\n",
       "      <td>파편이 튀어서 그 때 저도 응급실 가서 치료 받기도 했고…</td>\n",
       "      <td>응급실에 가셨다니 정말 놀랐어요. 아무 문제 없으신가요? 걱정 되네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5213</th>\n",
       "      <td>지금 상태가 너무 안 좋아서 학교 안 나가고 있어요.</td>\n",
       "      <td>상태가 더 안 좋아지셨군요. 걱정이 되네요.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5214</th>\n",
       "      <td>진짜 심해진 거 같긴 해요.</td>\n",
       "      <td>정말 힘드시겠어요. 지금도 증상이 심하신가요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5215</th>\n",
       "      <td>그런데 증상이 나빠진 거 같아.</td>\n",
       "      <td>너무 심하시면 병원을 다시 가보는 건 어떨까요?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1034 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     유저  \\\n",
       "0       제 감정이 이상해진 것 같아요. 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요.   \n",
       "1                              더 이상 내 감정을 내가 컨트롤 못 하겠어.   \n",
       "2                           하루종일 오르락내리락 롤러코스터 타는 기분이에요.   \n",
       "15    평소 다른 일을 할 때도 비슷해요. 생각한대로 안되면 화가 나고…그런 상황이 지속되...   \n",
       "16                                예전보다 화내는 게 과격해진 거 같아.   \n",
       "...                                                 ...   \n",
       "5196  그 사람이 응급실 의사한테 뭐라고 속닥거리니까, 저보고 갑자기 응급처치 끝났다고, ...   \n",
       "5197                   파편이 튀어서 그 때 저도 응급실 가서 치료 받기도 했고…   \n",
       "5213                     지금 상태가 너무 안 좋아서 학교 안 나가고 있어요.    \n",
       "5214                                   진짜 심해진 거 같긴 해요.    \n",
       "5215                                 그런데 증상이 나빠진 거 같아.    \n",
       "\n",
       "                                              챗봇  \n",
       "0                감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요.  \n",
       "1                         저도 그 기분 이해해요. 많이 힘드시죠?  \n",
       "2        그럴 때는 밥은 잘 먹었는지, 잠은 잘 잤는지 체크해보는 것도 좋아요.  \n",
       "15    화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요.  \n",
       "16           정말 힘드시겠어요. 화는 남에게도 스스로에게도 상처를 주잖아요.  \n",
       "...                                          ...  \n",
       "5196               응급실이 있어서 다행이네요. 큰 문제는 없으신 거죠?  \n",
       "5197     응급실에 가셨다니 정말 놀랐어요. 아무 문제 없으신가요? 걱정 되네요.  \n",
       "5213                    상태가 더 안 좋아지셨군요. 걱정이 되네요.  \n",
       "5214                   정말 힘드시겠어요. 지금도 증상이 심하신가요?  \n",
       "5215                  너무 심하시면 병원을 다시 가보는 건 어떨까요?  \n",
       "\n",
       "[1034 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filter = data.dropna()\n",
    "data_filter"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2. 데이터 전처리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어들을 따로 입력하기 위해  \n",
    "물음표 마침표 등을 띄워주는 전처리 작업을 거쳤다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.strip() #양 옆의 공백 제거\n",
    "\n",
    "  # 단어와 구두점(punctuation) 사이의 거리를 만듭니다.\n",
    "  # 예를 들어서 \"배고프다.\" => \"배고프다 .\"와 같이\n",
    "  # 온점 사이에 거리를 만듭니다.\n",
    "    sentence = re.sub(r\"([?.!,-])\", r\" \\1 \", sentence)\n",
    "\n",
    "  # (a-z, A-Z, \".\", \"?\", \"!\", \",\")를 제외한 모든 문자를 공백인 ' '로 대체합니다.\n",
    "  # sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence) 영어의 경우\n",
    "    sentence = re.sub(r\"([a-zA-Z]+)\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "\n",
    "  # sentence = re.sub(r\"^[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \" \", sentence) #제대로 된 한국어가 아닌 경우 ' ' 로 대체합니다\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수, 질문 : 1034 , 대답 : 1034\n",
      "\n",
      "\n",
      "질문 : 제 감정이 이상해진 것 같아요 . 남편만 보면 화가 치밀어 오르고 감정 조절이 안되요 .\n",
      "대답 : 감정이 조절이 안 될 때만큼 힘들 때는 없는 거 같아요 .\n",
      "\n",
      "질문 : 더 이상 내 감정을 내가 컨트롤 못 하겠어 .\n",
      "대답 : 저도 그 기분 이해해요 . 많이 힘드시죠 ?\n",
      "\n",
      "질문 : 하루종일 오르락내리락 롤러코스터 타는 기분이에요 .\n",
      "대답 : 그럴 때는 밥은 잘 먹었는지 , 잠은 잘 잤는지 체크해보는 것도 좋아요 .\n",
      "\n",
      "질문 : 평소 다른 일을 할 때도 비슷해요 . 생각한대로 안되면 화가 나고…그런 상황이 지속되면 폭발해버려요 .\n",
      "대답 : 화가 폭발할 것 같을 때는 그 자리를 피하는 것도 좋은 방법이라고 생각해요 .\n",
      "\n",
      "질문 : 예전보다 화내는 게 과격해진 거 같아 .\n",
      "대답 : 정말 힘드시겠어요 . 화는 남에게도 스스로에게도 상처를 주잖아요 .\n",
      "\n",
      "질문 : 화가 안 참아져 .\n",
      "대답 : 화가 너무 많이 날 때는 심호흡을 해보는 게 어떨까요 ? 씁 - 후 -\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Q = list(map(preprocess_sentence, data_filter['유저']))\n",
    "A = list(map(preprocess_sentence, data_filter['챗봇']))\n",
    "\n",
    "print(f'전체 샘플 수, 질문 : {len(Q)} , 대답 : {len(A)}\\n\\n')\n",
    "\n",
    "cnt = 0\n",
    "for a, b in zip(Q,A):\n",
    "    print(f'질문 : {a}')\n",
    "    print(f'대답 : {b}')\n",
    "    print()\n",
    "    cnt += 1\n",
    "    if cnt > 5:\n",
    "          break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(Q + A, target_vocab_size=2**13)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6358"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "\n",
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START_TOKEN의 번호 : [6358]\n",
      "END_TOKEN의 번호 : [6359]\n"
     ]
    }
   ],
   "source": [
    "print('START_TOKEN의 번호 :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKEN의 번호 :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6360\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = tokenizer.vocab_size + 2 # start 토큰과 end 토큰을 추가하였다.\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "질문 : 하루종일 오르락내리락 롤러코스터 타는 기분이에요 .\n",
      "대답 : 그럴 때는 밥은 잘 먹었는지 , 잠은 잘 잤는지 체크해보는 것도 좋아요 .\n",
      "\n",
      "정수 인코딩 후의 첫번째 질문 샘플: [227, 3325, 4830, 779, 1781, 1]\n",
      "정수 인코딩 후의 첫번째 답변 샘플: [34, 42, 4441, 20, 4681, 4, 604, 20, 2870, 2417, 23, 99, 1]\n"
     ]
    }
   ],
   "source": [
    "# 임의의 22번째 샘플에 대해서 정수 인코딩 작업을 수행.\n",
    "# 각 토큰을 고유한 정수로 변환\n",
    "\n",
    "print(f'질문 : {Q[2]}')\n",
    "print(f'대답 : {A[2]}\\n')\n",
    "    \n",
    "print('정수 인코딩 후의 첫번째 질문 샘플: {}'.format(tokenizer.encode(Q[2])))\n",
    "print('정수 인코딩 후의 첫번째 답변 샘플: {}'.format(tokenizer.encode(A[2])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "같은 마침표는 같은 숫자인 1 로 표기되는 것을 볼 수 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-3. 모델 학습"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3-1. split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  9.458413926499032\n",
      "문장길이 최대 :  25\n",
      "문장길이 표준편차 :  3.63755819096933\n",
      "pad_sequences maxlen :  16\n",
      "전체 문장의 0.960348162475822%가 maxlen 설정값 이내에 포함됩니다. \n"
     ]
    }
   ],
   "source": [
    "total_data_text = Q + A\n",
    "\n",
    "total_data_text = [tokenizer.encode(text) for text in total_data_text]\n",
    "\n",
    "# 텍스트데이터 문장길이의 리스트를 생성한 후\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 문장길이의 평균값, 최대값, 표준편차를 계산해 본다. \n",
    "print('문장길이 평균 : ', np.mean(num_tokens))\n",
    "print('문장길이 최대 : ', np.max(num_tokens))\n",
    "print('문장길이 표준편차 : ', np.std(num_tokens))\n",
    "\n",
    "# 예를들어, 최대 길이를 (평균 + 2*표준편차)로 한다면,  \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "maxlen = int(max_tokens)\n",
    "print('pad_sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}%가 maxlen 설정값 이내에 포함됩니다. '.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정수 인코딩, 최대 길이를 초과하는 샘플 제거, 패딩\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "    tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "    for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # 정수 인코딩 과정에서 시작 토큰과 종료 토큰을 추가\n",
    "        sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "        sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # 최대 길이 28 이하인 경우에만 데이터셋으로 허용\n",
    "        if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "            tokenized_inputs.append(sentence1)\n",
    "            tokenized_outputs.append(sentence2)\n",
    "  \n",
    "    # 최대 길이 28으로 모든 데이터셋을 패딩\n",
    "    tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "    tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "    return tokenized_inputs, tokenized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 6360\n",
      "필터링 후의 질문 샘플 개수: 1034\n",
      "필터링 후의 답변 샘플 개수: 1034\n"
     ]
    }
   ],
   "source": [
    "questions, answers = tokenize_and_filter(Q,A)\n",
    "\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(Q)))\n",
    "print('필터링 후의 답변 샘플 개수: {}'.format(len(A)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3-2. model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "    # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "\n",
    "    # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "      # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "      # softmax적용\n",
    "        attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "      # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "        output = tf.matmul(attention_weights, value)\n",
    "    return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "            'value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                      (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)\n",
    "\n",
    "# 인코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 두 개의 서브 레이어가 존재합니다.\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "        inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "      # 패딩 마스크 사용\n",
    "        padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "      # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "        attention = MultiHeadAttention(\n",
    "          d_model, num_heads, name=\"attention\")({\n",
    "              'query': inputs,\n",
    "              'key': inputs,\n",
    "              'value': inputs,\n",
    "              'mask': padding_mask\n",
    "          })\n",
    "\n",
    "      # 어텐션의 결과는 Dropout과 Layer Normalization이라는 훈련을 돕는 테크닉을 수행\n",
    "        attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "        attention = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "      # 두 번째 서브 레이어 : 2개의 완전연결층\n",
    "        outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "        outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "      # 완전연결층의 결과는 Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "        outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "        outputs = tf.keras.layers.LayerNormalization(\n",
    "          epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "        return tf.keras.Model(\n",
    "          inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # 패딩 마스크 사용\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layers만큼 쌓아올린 인코더의 층.\n",
    "    for i in range(num_layers):\n",
    "        outputs = encoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name=\"encoder_layer_{}\".format(i),\n",
    "        )([outputs, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "    inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "\n",
    "\n",
    "\n",
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "  # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "        })\n",
    "\n",
    "  # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "  # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # 패딩 마스크\n",
    "    padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "          units=units,\n",
    "          d_model=d_model,\n",
    "          num_heads=num_heads,\n",
    "          dropout=dropout,\n",
    "          name='decoder_layer_{}'.format(i),\n",
    "      )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)\n",
    "\n",
    "\n",
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # 인코더에서 패딩을 위한 마스크\n",
    "\n",
    "    enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "  # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "  # 디코더에서 패딩을 위한 마스크\n",
    "    dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # 인코더\n",
    "    enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "    )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # 디코더\n",
    "    dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "    )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " inputs (InputLayer)            [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " dec_inputs (InputLayer)        [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " enc_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " encoder (Functional)           (None, None, 256)    2682368     ['inputs[0][0]',                 \n",
      "                                                                  'enc_padding_mask[0][0]']       \n",
      "                                                                                                  \n",
      " look_ahead_mask (Lambda)       (None, 1, None, Non  0           ['dec_inputs[0][0]']             \n",
      "                                e)                                                                \n",
      "                                                                                                  \n",
      " dec_padding_mask (Lambda)      (None, 1, 1, None)   0           ['inputs[0][0]']                 \n",
      "                                                                                                  \n",
      " decoder (Functional)           (None, None, 256)    3209728     ['dec_inputs[0][0]',             \n",
      "                                                                  'encoder[0][0]',                \n",
      "                                                                  'look_ahead_mask[0][0]',        \n",
      "                                                                  'dec_padding_mask[0][0]']       \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, None, 6360)   1634520     ['decoder[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 7,526,616\n",
      "Trainable params: 7,526,616\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기, 노드\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32) #패딩마스크\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "     beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 17s 432ms/step - loss: 4.9961 - accuracy: 0.0616\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 6s 392ms/step - loss: 4.0996 - accuracy: 0.0824\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 5s 369ms/step - loss: 3.8326 - accuracy: 0.1325\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 5s 363ms/step - loss: 3.5484 - accuracy: 0.1576\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 5s 358ms/step - loss: 3.1805 - accuracy: 0.1925\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 5s 360ms/step - loss: 2.8175 - accuracy: 0.2247\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 5s 363ms/step - loss: 2.4900 - accuracy: 0.2624\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 5s 361ms/step - loss: 2.1783 - accuracy: 0.3078\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 5s 360ms/step - loss: 1.9014 - accuracy: 0.3556\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 5s 372ms/step - loss: 1.6237 - accuracy: 0.4144\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 5s 369ms/step - loss: 1.3678 - accuracy: 0.4597\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 5s 368ms/step - loss: 1.1418 - accuracy: 0.4970\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 5s 371ms/step - loss: 0.9216 - accuracy: 0.5395\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 5s 363ms/step - loss: 0.7336 - accuracy: 0.5667\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 5s 365ms/step - loss: 0.5779 - accuracy: 0.5968\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 5s 382ms/step - loss: 0.4448 - accuracy: 0.6206\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 5s 365ms/step - loss: 0.3396 - accuracy: 0.6377\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 5s 359ms/step - loss: 0.2588 - accuracy: 0.6529\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 5s 367ms/step - loss: 0.1954 - accuracy: 0.6609\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 5s 366ms/step - loss: 0.1464 - accuracy: 0.6654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d7c9092308>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-4. 데이터 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "  # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "  # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "    # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "    # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "    # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "      # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "  # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나 너무 힘들어\n",
      "출력 : 정말 당황스러우셨겠어요 . 하지만 너무 무리해서 생각해낼 필요는 없답니다 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'정말 당황스러우셨겠어요 . 하지만 너무 무리해서 생각해낼 필요는 없답니다 .'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('나 너무 힘들어')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 부트캠프 안하고 싶어 집에 갈래\n",
      "출력 : 그 마음 다 이해해요 . 그럴 땐 맛있는 음식을 먹으면 조금 나아져요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그 마음 다 이해해요 . 그럴 땐 맛있는 음식을 먹으면 조금 나아져요 .'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('부트캠프 안하고 싶어 집에 갈래')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다만 현재 들어가있는 정보들은 모두  \n",
    "위로해 주는 문답이기 때문에  \n",
    "어떤 말을 해도 위로하는 답변밖에 나오지 않는다는 한계가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나 오늘 행복해 사실\n",
      "출력 : 일이 많아 스트레스가 많겠어요 . 몸 상하지 않게 잘 챙겨 드세요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'일이 많아 스트레스가 많겠어요 . 몸 상하지 않게 잘 챙겨 드세요 .'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('나 오늘 행복해 사실')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 나 스트레스 안많은디\n",
      "출력 : 그 마음 다 이해해요 . 그럴 땐 맛있는 음식을 먹으면 조금 나아져요 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'그 마음 다 이해해요 . 그럴 땐 맛있는 음식을 먹으면 조금 나아져요 .'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('나 스트레스 안많은디')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. POTG\n",
    "## 3-1. 소감\n",
    "\n",
    "### \"너무 부족해요,, 이제 좀 더 잘 해볼게요 🥶\"\n",
    "지식이 너무 짧음을 매일 매일 실감합니다.  \n",
    "앞으로도 더 공부를 해야 할 것 같아요 ㅠㅠ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. 어려웠던 점과 극복방안"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 엑셀 파일을 열기 위해서는 'xlrd' 라는 라이브러리가 필요하다.  \n",
    "쉽게 `pip install xlrd` 로 설치한 후 이용해 주었다.\n",
    "\n",
    "2. SubwordTextEncoder API 를 사용하기 위해서  \n",
    " `tfds.deprecated.text` 라이브러리를 사용하려 했으나 계속 에러가 났다.  \n",
    " 이유는 버전문제였고 2.3+ 이하 버전은 `tfds.features.text` 를 사용해야 한다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4e373b8bba118244a329c5b4d875cb18b4349fdcd2551f3ef43ce953593cb0c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
